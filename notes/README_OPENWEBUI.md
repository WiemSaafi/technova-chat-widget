# Chat Widget avec Open WebUI

Ce chat widget a √©t√© modifi√© pour se connecter directement √† votre instance Open WebUI et utiliser vos mod√®les AI locaux.

## üöÄ Fonctionnalit√©s

- **Connexion directe √† Open WebUI** : Communique avec votre instance Open WebUI locale
- **Support multi-mod√®les** : Compatible avec tous les mod√®les support√©s par Open WebUI (Llama, Mistral, CodeLlama, etc.)
- **Configuration flexible** : Fichier de configuration s√©par√© pour faciliter la personnalisation
- **Historique des conversations** : Maintient le contexte de la conversation
- **Indicateur de frappe** : Animation pendant le traitement de la requ√™te
- **Gestion d'erreurs** : Messages d'erreur personnalis√©s et gestion des timeouts
- **Responsive** : Fonctionne sur desktop et mobile

## üìã Pr√©requis

1. **Open WebUI install√© et en cours d'ex√©cution**
   - Suivez le guide d'installation : https://docs.openwebui.com/
   - V√©rifiez que votre instance est accessible (par d√©faut : http://localhost:8080)

2. **Mod√®les AI install√©s**
   - T√©l√©chargez au moins un mod√®le via Open WebUI
   - Notez le nom exact du mod√®le (ex: `llama3.2:latest`)

## üîß Installation

1. **Clonez ou t√©l√©chargez les fichiers**
   ```bash
   git clone https://github.com/anantrp/chat-widget.git
   cd chat-widget
   ```

2. **Configurez Open WebUI**
   - Ouvrez le fichier `config.js`
   - Modifiez les param√®tres selon votre configuration :

   ```javascript
   const OPENWEBUI_CONFIG = {
     // URL de votre instance Open WebUI
     openWebUIUrl: 'http://localhost:8080',
     
     // Cl√© API si n√©cessaire (laissez vide si pas d'authentification)
     apiKey: '',
     
     // Mod√®le √† utiliser (v√©rifiez le nom exact dans Open WebUI)
     model: 'llama3.2:latest',
     
     // Param√®tres de g√©n√©ration
     maxTokens: 1000,
     temperature: 0.7,
     
     // Message syst√®me pour d√©finir le comportement de l'IA
     systemMessage: 'Vous √™tes un assistant intelligent et serviable.',
   };
   ```

3. **Int√©grez le widget √† votre site**
   ```html
   <!DOCTYPE html>
   <html>
   <head>
     <title>Mon Site</title>
     <script src="./config.js"></script>
     <script async src="./chat-widget.js"></script>
   </head>
   <body>
     <!-- Votre contenu -->
   </body>
   </html>
   ```

## ‚öôÔ∏è Configuration

### Param√®tres principaux

| Param√®tre | Description | Exemple |
|-----------|-------------|---------|
| `openWebUIUrl` | URL de votre instance Open WebUI | `http://localhost:8080` |
| `apiKey` | Cl√© API si authentification requise | `sk-...` ou `''` |
| `model` | Nom du mod√®le √† utiliser | `llama3.2:latest` |
| `maxTokens` | Nombre maximum de tokens √† g√©n√©rer | `1000` |
| `temperature` | Cr√©ativit√© des r√©ponses (0-1) | `0.7` |
| `systemMessage` | Instructions syst√®me pour l'IA | `Vous √™tes un assistant...` |

### Mod√®les support√©s

Le widget fonctionne avec tous les mod√®les support√©s par Open WebUI :
- **Llama** : `llama3.2:latest`, `llama3.1:8b`, etc.
- **Mistral** : `mistral:latest`, `mistral:7b`, etc.
- **CodeLlama** : `codellama:latest`, `codellama:7b`, etc.
- **Autres** : Phi, Gemma, et tous les mod√®les compatibles Ollama

### Configuration avanc√©e

```javascript
const OPENWEBUI_CONFIG = {
  // Configuration de base
  openWebUIUrl: 'http://localhost:8080',
  model: 'llama3.2:latest',
  
  // Param√®tres avanc√©s
  stream: false, // Streaming des r√©ponses
  timeout: 30000, // Timeout en millisecondes
  
  // Messages d'erreur personnalis√©s
  errorMessages: {
    networkError: 'Impossible de se connecter au serveur.',
    serverError: 'Erreur du serveur. Veuillez r√©essayer.',
    timeout: 'La requ√™te a pris trop de temps.',
    general: 'Erreur technique. Veuillez r√©essayer.'
  }
};
```

## üîç D√©pannage

### Probl√®mes courants

1. **"Impossible de se connecter au serveur"**
   - V√©rifiez que Open WebUI est en cours d'ex√©cution
   - V√©rifiez l'URL dans `config.js`
   - V√©rifiez les param√®tres CORS d'Open WebUI

2. **"Erreur HTTP: 404"**
   - V√©rifiez que l'endpoint API est correct
   - Certaines versions d'Open WebUI peuvent avoir des endpoints diff√©rents

3. **"Erreur HTTP: 401"**
   - Authentification requise : ajoutez votre cl√© API dans `config.js`

4. **"Erreur HTTP: 400"**
   - V√©rifiez que le nom du mod√®le est correct
   - V√©rifiez que le mod√®le est bien install√© dans Open WebUI

### Configuration CORS

Si vous rencontrez des erreurs CORS, vous devez configurer Open WebUI pour accepter les requ√™tes depuis votre domaine :

```bash
# Exemple avec Docker
docker run -d -p 8080:8080 \
  -e WEBUI_AUTH=false \
  -e CORS_ALLOW_ORIGIN="*" \
  --name open-webui \
  ghcr.io/open-webui/open-webui:main
```

### V√©rification de la configuration

1. **Testez l'acc√®s √† Open WebUI**
   ```bash
   curl http://localhost:8080/api/models
   ```

2. **V√©rifiez la console du navigateur**
   - Ouvrez les outils de d√©veloppement (F12)
   - Regardez les erreurs dans la console

3. **Testez l'API directement**
   ```bash
   curl -X POST http://localhost:8080/api/chat/completions \
     -H "Content-Type: application/json" \
     -d '{
       "model": "llama3.2:latest",
       "messages": [{"role": "user", "content": "Hello"}],
       "max_tokens": 100
     }'
   ```

## üé® Personnalisation

### Style CSS

Le widget utilise Tailwind CSS. Vous pouvez personnaliser l'apparence en modifiant les classes CSS dans `chat-widget.js`.

### Messages syst√®me

Personnalisez le comportement de l'IA en modifiant le `systemMessage` :

```javascript
systemMessage: 'Vous √™tes un expert en d√©veloppement web. R√©pondez de mani√®re technique et pr√©cise.'
```

### Gestion des erreurs

Personnalisez les messages d'erreur dans `config.js` :

```javascript
errorMessages: {
  networkError: 'Probl√®me de connexion r√©seau',
  serverError: 'Le serveur rencontre des difficult√©s',
  timeout: 'Temps de r√©ponse trop long',
  general: 'Une erreur est survenue'
}
```

## üìö API Reference

### Endpoints utilis√©s

- `POST /api/chat/completions` : Envoie des messages et re√ßoit des r√©ponses
- `GET /api/models` : Liste les mod√®les disponibles (pour v√©rification)

### Format des messages

```javascript
{
  "model": "llama3.2:latest",
  "messages": [
    {"role": "system", "content": "Vous √™tes un assistant..."},
    {"role": "user", "content": "Bonjour"},
    {"role": "assistant", "content": "Bonjour ! Comment puis-je vous aider ?"}
  ],
  "max_tokens": 1000,
  "temperature": 0.7,
  "stream": false
}
```

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† :
- Signaler des bugs
- Proposer des am√©liorations
- Soumettre des pull requests

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## üîó Liens utiles

- [Open WebUI Documentation](https://docs.openwebui.com/)
- [Ollama Models](https://ollama.ai/library)
- [Tailwind CSS](https://tailwindcss.com/)
- [Projet original](https://github.com/anantrp/chat-widget)
